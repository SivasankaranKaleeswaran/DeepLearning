{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924386f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T16:49:35.829097Z",
     "iopub.status.busy": "2023-10-30T16:49:35.828168Z",
     "iopub.status.idle": "2023-10-30T16:49:42.511672Z",
     "shell.execute_reply": "2023-10-30T16:49:42.510776Z",
     "shell.execute_reply.started": "2023-10-30T16:49:35.829064Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file_path = '/kaggle/input/nyc-yello-taxi/yellow_tripdata_2020-01.parquet'\n",
    "df = pd.read_parquet(parquet_file_path, engine='pyarrow')\n",
    "\n",
    "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "df['pickup_date'] = df['tpep_pickup_datetime'].dt.date\n",
    "df=df[df['tpep_pickup_datetime'].dt.month==1]\n",
    "df['time_taken'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b815206d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T16:49:42.513704Z",
     "iopub.status.busy": "2023-10-30T16:49:42.513398Z",
     "iopub.status.idle": "2023-10-30T16:49:46.140769Z",
     "shell.execute_reply": "2023-10-30T16:49:46.139827Z",
     "shell.execute_reply.started": "2023-10-30T16:49:42.513679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>238</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.27</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>4.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>239</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>7.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>246</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.55</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>16.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>161</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>24.95</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>25.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.76</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>5.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339561</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.56</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>21.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339562</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>163</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.76</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>14.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339563</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>164</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.16</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339564</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.56</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>16.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339565</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>9.633333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4485618 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         passenger_count  trip_distance  RatecodeID  PULocationID  \\\n",
       "0                    1.0           1.20         1.0           238   \n",
       "1                    1.0           1.20         1.0           239   \n",
       "10                   2.0           2.40         1.0           246   \n",
       "12                   1.0           3.30         1.0           161   \n",
       "13                   1.0           1.07         1.0            43   \n",
       "...                  ...            ...         ...           ...   \n",
       "6339561              1.0           4.11         1.0            48   \n",
       "6339562              1.0           2.10         1.0           163   \n",
       "6339563              1.0           2.13         1.0           164   \n",
       "6339564              1.0           2.55         1.0            79   \n",
       "6339565              1.0           1.61         1.0           100   \n",
       "\n",
       "         DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0                 239             1          6.0    3.0      0.5        1.47   \n",
       "1                 238             1          7.0    3.0      0.5        1.50   \n",
       "10                 79             1         12.0    3.0      0.5        1.75   \n",
       "12                144             1         17.0    3.0      0.5        4.15   \n",
       "13                239             1          6.0    0.5      0.5        1.96   \n",
       "...               ...           ...          ...    ...      ...         ...   \n",
       "6339561            75             1         17.5    0.5      0.5        4.26   \n",
       "6339562           246             1         11.0    0.5      0.5        2.96   \n",
       "6339563            79             1         13.0    0.5      0.5        3.36   \n",
       "6339564            68             1         12.5    0.5      0.5        3.26   \n",
       "6339565           142             2          8.5    0.5      0.5        0.00   \n",
       "\n",
       "         improvement_surcharge  total_amount  congestion_surcharge  \\\n",
       "0                          0.3         11.27                   2.5   \n",
       "1                          0.3         12.30                   2.5   \n",
       "10                         0.3         17.55                   2.5   \n",
       "12                         0.3         24.95                   2.5   \n",
       "13                         0.3         11.76                   2.5   \n",
       "...                        ...           ...                   ...   \n",
       "6339561                    0.3         25.56                   2.5   \n",
       "6339562                    0.3         17.76                   2.5   \n",
       "6339563                    0.3         20.16                   2.5   \n",
       "6339564                    0.3         19.56                   2.5   \n",
       "6339565                    0.3         12.30                   2.5   \n",
       "\n",
       "        pickup_date  time_taken  \n",
       "0        2020-01-01    4.800000  \n",
       "1        2020-01-01    7.416667  \n",
       "10       2020-01-01   16.866667  \n",
       "12       2020-01-01   25.283333  \n",
       "13       2020-01-01    5.616667  \n",
       "...             ...         ...  \n",
       "6339561  2020-01-31   21.500000  \n",
       "6339562  2020-01-31   14.233333  \n",
       "6339563  2020-01-31   19.000000  \n",
       "6339564  2020-01-31   16.283333  \n",
       "6339565  2020-01-31    9.633333  \n",
       "\n",
       "[4485618 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df\n",
    "df2['pickup_date'] = pd.to_datetime(df2['pickup_date'], format='%Y-%m-%d')\n",
    "columns_to_remove = ['VendorID', 'store_and_fwd_flag', 'airport_fee','tpep_pickup_datetime','tpep_dropoff_datetime', 'tolls_amount']\n",
    "df2.drop(columns=columns_to_remove, inplace=True)\n",
    "df2.dropna(how='any', inplace=True)\n",
    "df2 = df2[df2['passenger_count'] != 0]\n",
    "df2 = df2[df2['trip_distance'] > 1]\n",
    "df2 = df2[df2['time_taken'] > 1]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319a1008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T16:49:46.142215Z",
     "iopub.status.busy": "2023-10-30T16:49:46.141933Z",
     "iopub.status.idle": "2023-10-30T16:49:46.889705Z",
     "shell.execute_reply": "2023-10-30T16:49:46.888533Z",
     "shell.execute_reply.started": "2023-10-30T16:49:46.142189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1008.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1013.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>7.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1010.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1003.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>7.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1010.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1016.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>2.6</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>13.4</td>\n",
       "      <td>1016.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1038.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1034.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>15.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>19.4</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1018.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1025.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1020.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1015.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1034.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-01-18</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1028.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1008.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1024.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1030.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1031.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1029.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>11.7</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1027.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>6.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.9</td>\n",
       "      <td>1014.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>8.9</td>\n",
       "      <td>14.9</td>\n",
       "      <td>1009.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>7.2</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1010.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1018.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>10.7</td>\n",
       "      <td>1026.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1026.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_date  tavg  tmin  tmax  wspd    pres\n",
       "0   2020-01-01   3.6   1.7   5.0  17.3  1008.2\n",
       "1   2020-01-02   4.7   0.6   8.9  12.4  1013.9\n",
       "2   2020-01-03   7.6   6.7   8.3   8.4  1010.2\n",
       "3   2020-01-04   8.2   6.7   9.4   5.7  1003.7\n",
       "4   2020-01-05   4.6   2.8   7.2   8.2  1010.1\n",
       "6   2020-01-07   4.7   3.3   6.7  10.4  1016.2\n",
       "7   2020-01-08   2.6  -0.6   6.7  13.4  1016.6\n",
       "8   2020-01-09  -0.6  -3.3   2.2   9.1  1038.8\n",
       "9   2020-01-10   6.4   1.1  11.7  14.5  1034.5\n",
       "11  2020-01-12  15.7   9.4  19.4  15.5  1018.1\n",
       "13  2020-01-14   6.3   5.0   7.8   9.9  1025.3\n",
       "14  2020-01-15   8.2   6.1  11.7   9.0  1020.9\n",
       "15  2020-01-16   6.7   2.2  10.0  10.6  1015.1\n",
       "16  2020-01-17  -1.2  -3.9   2.2   8.8  1034.7\n",
       "17  2020-01-18  -2.2  -5.0   2.8  10.2  1028.5\n",
       "18  2020-01-19   4.0   0.6   7.2  10.2  1008.9\n",
       "19  2020-01-20  -1.7  -5.0   2.8   9.1  1024.2\n",
       "20  2020-01-21  -1.5  -4.4   3.3   8.9  1030.8\n",
       "21  2020-01-22   1.0  -2.2   5.6   8.1  1031.3\n",
       "22  2020-01-23   3.5   0.0   7.2   7.5  1029.4\n",
       "23  2020-01-24   6.5   3.3  11.7  10.6  1027.4\n",
       "24  2020-01-25   6.6   4.4  10.0  22.9  1014.5\n",
       "25  2020-01-26   5.5   3.3   8.9  14.9  1009.3\n",
       "27  2020-01-28   4.9   3.9   7.2   6.3  1010.2\n",
       "28  2020-01-29   3.5   1.1   7.2   7.8  1018.2\n",
       "29  2020-01-30   1.3  -1.7   4.4  10.7  1026.3\n",
       "30  2020-01-31   4.3   2.2   6.1   7.5  1026.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_excel('/kaggle/input/nyc-yello-taxi/weather.xlsx')\n",
    "df1['pickup_date'] = pd.to_datetime(df1['pickup_date'], format='%Y-%m-%d')\n",
    "\n",
    "columns_to_remove = ['prcp', 'snow', 'wdir','wpgt', 'tsun']\n",
    "df1.drop(columns=columns_to_remove, inplace=True)\n",
    "df1.dropna(how='any', inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20b3954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T16:49:46.891185Z",
     "iopub.status.busy": "2023-10-30T16:49:46.890813Z",
     "iopub.status.idle": "2023-10-30T16:49:47.726458Z",
     "shell.execute_reply": "2023-10-30T16:49:47.725524Z",
     "shell.execute_reply.started": "2023-10-30T16:49:46.891140Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>238</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.27</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1008.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>239</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>7.416667</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1008.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>246</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.55</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>16.866667</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1008.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>161</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>24.95</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>25.283333</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1008.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.76</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>5.616667</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1008.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4485613</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.56</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4485614</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>163</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.76</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>14.233333</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4485615</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>164</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.16</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4485616</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.56</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>16.283333</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4485617</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>9.633333</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1026.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4485618 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         passenger_count  trip_distance  RatecodeID  PULocationID  \\\n",
       "0                    1.0           1.20         1.0           238   \n",
       "1                    1.0           1.20         1.0           239   \n",
       "2                    2.0           2.40         1.0           246   \n",
       "3                    1.0           3.30         1.0           161   \n",
       "4                    1.0           1.07         1.0            43   \n",
       "...                  ...            ...         ...           ...   \n",
       "4485613              1.0           4.11         1.0            48   \n",
       "4485614              1.0           2.10         1.0           163   \n",
       "4485615              1.0           2.13         1.0           164   \n",
       "4485616              1.0           2.55         1.0            79   \n",
       "4485617              1.0           1.61         1.0           100   \n",
       "\n",
       "         DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0                 239             1          6.0    3.0      0.5        1.47   \n",
       "1                 238             1          7.0    3.0      0.5        1.50   \n",
       "2                  79             1         12.0    3.0      0.5        1.75   \n",
       "3                 144             1         17.0    3.0      0.5        4.15   \n",
       "4                 239             1          6.0    0.5      0.5        1.96   \n",
       "...               ...           ...          ...    ...      ...         ...   \n",
       "4485613            75             1         17.5    0.5      0.5        4.26   \n",
       "4485614           246             1         11.0    0.5      0.5        2.96   \n",
       "4485615            79             1         13.0    0.5      0.5        3.36   \n",
       "4485616            68             1         12.5    0.5      0.5        3.26   \n",
       "4485617           142             2          8.5    0.5      0.5        0.00   \n",
       "\n",
       "         improvement_surcharge  total_amount  congestion_surcharge  \\\n",
       "0                          0.3         11.27                   2.5   \n",
       "1                          0.3         12.30                   2.5   \n",
       "2                          0.3         17.55                   2.5   \n",
       "3                          0.3         24.95                   2.5   \n",
       "4                          0.3         11.76                   2.5   \n",
       "...                        ...           ...                   ...   \n",
       "4485613                    0.3         25.56                   2.5   \n",
       "4485614                    0.3         17.76                   2.5   \n",
       "4485615                    0.3         20.16                   2.5   \n",
       "4485616                    0.3         19.56                   2.5   \n",
       "4485617                    0.3         12.30                   2.5   \n",
       "\n",
       "        pickup_date  time_taken  tavg  tmin  tmax  wspd    pres  \n",
       "0        2020-01-01    4.800000   3.6   1.7   5.0  17.3  1008.2  \n",
       "1        2020-01-01    7.416667   3.6   1.7   5.0  17.3  1008.2  \n",
       "2        2020-01-01   16.866667   3.6   1.7   5.0  17.3  1008.2  \n",
       "3        2020-01-01   25.283333   3.6   1.7   5.0  17.3  1008.2  \n",
       "4        2020-01-01    5.616667   3.6   1.7   5.0  17.3  1008.2  \n",
       "...             ...         ...   ...   ...   ...   ...     ...  \n",
       "4485613  2020-01-31   21.500000   4.3   2.2   6.1   7.5  1026.0  \n",
       "4485614  2020-01-31   14.233333   4.3   2.2   6.1   7.5  1026.0  \n",
       "4485615  2020-01-31   19.000000   4.3   2.2   6.1   7.5  1026.0  \n",
       "4485616  2020-01-31   16.283333   4.3   2.2   6.1   7.5  1026.0  \n",
       "4485617  2020-01-31    9.633333   4.3   2.2   6.1   7.5  1026.0  \n",
       "\n",
       "[4485618 rows x 20 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = df2.merge(df1, left_on='pickup_date', right_on='pickup_date', how='left')\n",
    "\n",
    "columns_to_populate = ['tavg', 'tmin', 'tmax', 'wspd']\n",
    "for col in columns_to_populate:\n",
    "    merged_df[col].fillna(merged_df[f'{col}'], inplace=True)\n",
    "#merged_df=merged_df[merged_df['pickup_date'].dt.day<=10]\n",
    "#merged_df = merged_df[merged_df['tavg'] > 0]\n",
    "#merged_df = merged_df[merged_df['wspd'] > 0]\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acfc278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T16:49:47.729241Z",
     "iopub.status.busy": "2023-10-30T16:49:47.728896Z",
     "iopub.status.idle": "2023-10-30T16:49:48.657760Z",
     "shell.execute_reply": "2023-10-30T16:49:48.656739Z",
     "shell.execute_reply.started": "2023-10-30T16:49:47.729211Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_columns = ['trip_distance','PULocationID', 'DOLocationID', 'time_taken', 'tavg', 'pres']\n",
    "\n",
    "new_df = merged_df[selected_columns]\n",
    "#new_df=new_df[new_df['PULocationID'].isin([132, 161, 237, 236, 186, 162, 230, 142, 48])]\n",
    "#new_df=new_df[new_df['DOLocationID'].isin([236,237,161,239,142, 238, 162])]\n",
    "new_df = new_df.dropna(subset=['tavg', 'pres'])\n",
    "new_df = new_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d222767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T16:49:48.659355Z",
     "iopub.status.busy": "2023-10-30T16:49:48.659024Z",
     "iopub.status.idle": "2023-10-30T16:49:49.732886Z",
     "shell.execute_reply": "2023-10-30T16:49:49.731879Z",
     "shell.execute_reply.started": "2023-10-30T16:49:48.659327Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data - X_train shape: (1409053, 5) y_train shape: (1409053,)\n",
      "Testing data - X_test shape: (352264, 5) y_test shape: (352264,)\n",
      "         trip_distance  PULocationID  DOLocationID  time_taken  tavg    pres\n",
      "0                 1.40           186           137   12.916667   6.4  1034.5\n",
      "2                 2.56           107            87   15.550000   6.6  1014.5\n",
      "4                 2.58           236           161   17.900000   2.6  1016.6\n",
      "5                 2.47           261            90   11.550000  -1.2  1034.7\n",
      "7                 3.64           246           239   16.900000   4.0  1008.9\n",
      "...                ...           ...           ...         ...   ...     ...\n",
      "3923024           2.38            74           159   10.883333  -1.2  1034.7\n",
      "3923026           1.01           141           162   11.966667   6.7  1015.1\n",
      "3923027           2.50           158            50   11.033333   4.0  1008.9\n",
      "3923028           2.60            75           142   16.433333   3.5  1029.4\n",
      "3923029           2.60           264           141   12.666667  -1.2  1034.7\n",
      "\n",
      "[1761317 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "p_27 = new_df['time_taken'].quantile(0.27)\n",
    "p_72 = new_df['time_taken'].quantile(0.72)\n",
    "new_df = new_df[(new_df['time_taken'] > p_27) & (new_df['time_taken'] < p_72)]\n",
    "#new_df.sort_values('DOLocationID', ascending=False, inplace=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = new_df.drop(columns=['time_taken'])\n",
    "y = new_df['time_taken']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training data - X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"Testing data - X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b3021e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T16:54:39.552121Z",
     "iopub.status.busy": "2023-10-30T16:54:39.551377Z",
     "iopub.status.idle": "2023-10-31T03:17:54.258033Z",
     "shell.execute_reply": "2023-10-31T03:17:54.256995Z",
     "shell.execute_reply.started": "2023-10-30T16:54:39.552085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 5.1022 - mean_absolute_error: 1.8641 - val_loss: 4.9230 - val_mean_absolute_error: 1.8353\n",
      "Epoch 2/100\n",
      "44033/44033 [==============================] - 173s 4ms/step - loss: 4.9426 - mean_absolute_error: 1.8440 - val_loss: 4.9177 - val_mean_absolute_error: 1.8372\n",
      "Epoch 3/100\n",
      "44033/44033 [==============================] - 174s 4ms/step - loss: 4.9197 - mean_absolute_error: 1.8394 - val_loss: 4.8999 - val_mean_absolute_error: 1.8320\n",
      "Epoch 4/100\n",
      "44033/44033 [==============================] - 173s 4ms/step - loss: 4.9027 - mean_absolute_error: 1.8358 - val_loss: 4.8810 - val_mean_absolute_error: 1.8218\n",
      "Epoch 5/100\n",
      "44033/44033 [==============================] - 173s 4ms/step - loss: 4.8889 - mean_absolute_error: 1.8330 - val_loss: 4.9082 - val_mean_absolute_error: 1.8449\n",
      "Epoch 6/100\n",
      "44033/44033 [==============================] - 172s 4ms/step - loss: 4.8784 - mean_absolute_error: 1.8308 - val_loss: 4.8687 - val_mean_absolute_error: 1.8337\n",
      "Epoch 7/100\n",
      "44033/44033 [==============================] - 172s 4ms/step - loss: 4.8671 - mean_absolute_error: 1.8283 - val_loss: 4.8422 - val_mean_absolute_error: 1.8282\n",
      "Epoch 8/100\n",
      "44033/44033 [==============================] - 170s 4ms/step - loss: 4.8574 - mean_absolute_error: 1.8260 - val_loss: 4.8584 - val_mean_absolute_error: 1.8134\n",
      "Epoch 9/100\n",
      "44033/44033 [==============================] - 171s 4ms/step - loss: 4.8493 - mean_absolute_error: 1.8242 - val_loss: 4.8645 - val_mean_absolute_error: 1.8293\n",
      "Epoch 10/100\n",
      "44033/44033 [==============================] - 170s 4ms/step - loss: 4.8413 - mean_absolute_error: 1.8224 - val_loss: 4.8243 - val_mean_absolute_error: 1.8251\n",
      "Epoch 11/100\n",
      "44033/44033 [==============================] - 170s 4ms/step - loss: 4.8336 - mean_absolute_error: 1.8207 - val_loss: 4.8262 - val_mean_absolute_error: 1.8155\n",
      "Epoch 12/100\n",
      "44033/44033 [==============================] - 170s 4ms/step - loss: 4.8270 - mean_absolute_error: 1.8192 - val_loss: 4.7961 - val_mean_absolute_error: 1.8162\n",
      "Epoch 13/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.8199 - mean_absolute_error: 1.8177 - val_loss: 4.8863 - val_mean_absolute_error: 1.8038\n",
      "Epoch 14/100\n",
      "44033/44033 [==============================] - 170s 4ms/step - loss: 4.8126 - mean_absolute_error: 1.8159 - val_loss: 4.7927 - val_mean_absolute_error: 1.8123\n",
      "Epoch 15/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.8054 - mean_absolute_error: 1.8143 - val_loss: 4.7825 - val_mean_absolute_error: 1.8093\n",
      "Epoch 16/100\n",
      "44033/44033 [==============================] - 170s 4ms/step - loss: 4.7988 - mean_absolute_error: 1.8129 - val_loss: 4.8000 - val_mean_absolute_error: 1.8200\n",
      "Epoch 17/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.7911 - mean_absolute_error: 1.8111 - val_loss: 4.7806 - val_mean_absolute_error: 1.8038\n",
      "Epoch 18/100\n",
      "44033/44033 [==============================] - 170s 4ms/step - loss: 4.7839 - mean_absolute_error: 1.8094 - val_loss: 4.7805 - val_mean_absolute_error: 1.8032\n",
      "Epoch 19/100\n",
      "44033/44033 [==============================] - 170s 4ms/step - loss: 4.7762 - mean_absolute_error: 1.8078 - val_loss: 4.7836 - val_mean_absolute_error: 1.8006\n",
      "Epoch 20/100\n",
      "44033/44033 [==============================] - 171s 4ms/step - loss: 4.7693 - mean_absolute_error: 1.8059 - val_loss: 4.7584 - val_mean_absolute_error: 1.8013\n",
      "Epoch 21/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.7612 - mean_absolute_error: 1.8042 - val_loss: 4.7702 - val_mean_absolute_error: 1.8117\n",
      "Epoch 22/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.7545 - mean_absolute_error: 1.8026 - val_loss: 4.7394 - val_mean_absolute_error: 1.8010\n",
      "Epoch 23/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.7485 - mean_absolute_error: 1.8013 - val_loss: 4.7523 - val_mean_absolute_error: 1.8057\n",
      "Epoch 24/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.7418 - mean_absolute_error: 1.7995 - val_loss: 4.7477 - val_mean_absolute_error: 1.8018\n",
      "Epoch 25/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.7347 - mean_absolute_error: 1.7979 - val_loss: 4.7407 - val_mean_absolute_error: 1.7944\n",
      "Epoch 26/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.7297 - mean_absolute_error: 1.7967 - val_loss: 4.7338 - val_mean_absolute_error: 1.7968\n",
      "Epoch 27/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.7233 - mean_absolute_error: 1.7953 - val_loss: 4.7298 - val_mean_absolute_error: 1.8020\n",
      "Epoch 28/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.7191 - mean_absolute_error: 1.7943 - val_loss: 4.7217 - val_mean_absolute_error: 1.8023\n",
      "Epoch 29/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.7138 - mean_absolute_error: 1.7930 - val_loss: 4.7077 - val_mean_absolute_error: 1.7837\n",
      "Epoch 30/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.7092 - mean_absolute_error: 1.7920 - val_loss: 4.7058 - val_mean_absolute_error: 1.7843\n",
      "Epoch 31/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.7037 - mean_absolute_error: 1.7908 - val_loss: 4.7191 - val_mean_absolute_error: 1.7826\n",
      "Epoch 32/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.6990 - mean_absolute_error: 1.7897 - val_loss: 4.6942 - val_mean_absolute_error: 1.7916\n",
      "Epoch 33/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.6941 - mean_absolute_error: 1.7884 - val_loss: 4.6946 - val_mean_absolute_error: 1.7852\n",
      "Epoch 34/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.6903 - mean_absolute_error: 1.7877 - val_loss: 4.7049 - val_mean_absolute_error: 1.7810\n",
      "Epoch 35/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6863 - mean_absolute_error: 1.7866 - val_loss: 4.6890 - val_mean_absolute_error: 1.7909\n",
      "Epoch 36/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6816 - mean_absolute_error: 1.7856 - val_loss: 4.6717 - val_mean_absolute_error: 1.7805\n",
      "Epoch 37/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6785 - mean_absolute_error: 1.7848 - val_loss: 4.6793 - val_mean_absolute_error: 1.7809\n",
      "Epoch 38/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6742 - mean_absolute_error: 1.7837 - val_loss: 4.6667 - val_mean_absolute_error: 1.7840\n",
      "Epoch 39/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6707 - mean_absolute_error: 1.7831 - val_loss: 4.6901 - val_mean_absolute_error: 1.7848\n",
      "Epoch 40/100\n",
      "44033/44033 [==============================] - 169s 4ms/step - loss: 4.6675 - mean_absolute_error: 1.7823 - val_loss: 4.6730 - val_mean_absolute_error: 1.7855\n",
      "Epoch 41/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6640 - mean_absolute_error: 1.7813 - val_loss: 4.6796 - val_mean_absolute_error: 1.7749\n",
      "Epoch 42/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6599 - mean_absolute_error: 1.7804 - val_loss: 4.6723 - val_mean_absolute_error: 1.7767\n",
      "Epoch 43/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6561 - mean_absolute_error: 1.7794 - val_loss: 4.6705 - val_mean_absolute_error: 1.7834\n",
      "Epoch 44/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6537 - mean_absolute_error: 1.7790 - val_loss: 4.6707 - val_mean_absolute_error: 1.7897\n",
      "Epoch 45/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6503 - mean_absolute_error: 1.7781 - val_loss: 4.6570 - val_mean_absolute_error: 1.7838\n",
      "Epoch 46/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6481 - mean_absolute_error: 1.7778 - val_loss: 4.6664 - val_mean_absolute_error: 1.7728\n",
      "Epoch 47/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6435 - mean_absolute_error: 1.7766 - val_loss: 4.6647 - val_mean_absolute_error: 1.7728\n",
      "Epoch 48/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.6401 - mean_absolute_error: 1.7759 - val_loss: 4.7099 - val_mean_absolute_error: 1.7731\n",
      "Epoch 49/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6384 - mean_absolute_error: 1.7754 - val_loss: 4.6497 - val_mean_absolute_error: 1.7767\n",
      "Epoch 50/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.6355 - mean_absolute_error: 1.7749 - val_loss: 4.6855 - val_mean_absolute_error: 1.7770\n",
      "Epoch 51/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6326 - mean_absolute_error: 1.7742 - val_loss: 4.6446 - val_mean_absolute_error: 1.7801\n",
      "Epoch 52/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.6296 - mean_absolute_error: 1.7732 - val_loss: 4.6620 - val_mean_absolute_error: 1.7848\n",
      "Epoch 53/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6274 - mean_absolute_error: 1.7729 - val_loss: 4.6706 - val_mean_absolute_error: 1.7900\n",
      "Epoch 54/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6240 - mean_absolute_error: 1.7722 - val_loss: 4.6361 - val_mean_absolute_error: 1.7702\n",
      "Epoch 55/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6212 - mean_absolute_error: 1.7714 - val_loss: 4.6691 - val_mean_absolute_error: 1.7688\n",
      "Epoch 56/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.6183 - mean_absolute_error: 1.7706 - val_loss: 4.6471 - val_mean_absolute_error: 1.7761\n",
      "Epoch 57/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6155 - mean_absolute_error: 1.7701 - val_loss: 4.6572 - val_mean_absolute_error: 1.7855\n",
      "Epoch 58/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6144 - mean_absolute_error: 1.7696 - val_loss: 4.6512 - val_mean_absolute_error: 1.7748\n",
      "Epoch 59/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.6108 - mean_absolute_error: 1.7687 - val_loss: 4.6320 - val_mean_absolute_error: 1.7696\n",
      "Epoch 60/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6083 - mean_absolute_error: 1.7683 - val_loss: 4.6340 - val_mean_absolute_error: 1.7686\n",
      "Epoch 61/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.6063 - mean_absolute_error: 1.7676 - val_loss: 4.6213 - val_mean_absolute_error: 1.7672\n",
      "Epoch 62/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.6037 - mean_absolute_error: 1.7669 - val_loss: 4.6186 - val_mean_absolute_error: 1.7685\n",
      "Epoch 63/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.6009 - mean_absolute_error: 1.7665 - val_loss: 4.6329 - val_mean_absolute_error: 1.7671\n",
      "Epoch 64/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5986 - mean_absolute_error: 1.7660 - val_loss: 4.6315 - val_mean_absolute_error: 1.7654\n",
      "Epoch 65/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5977 - mean_absolute_error: 1.7658 - val_loss: 4.6124 - val_mean_absolute_error: 1.7689\n",
      "Epoch 66/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.5943 - mean_absolute_error: 1.7648 - val_loss: 4.6409 - val_mean_absolute_error: 1.7623\n",
      "Epoch 67/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5923 - mean_absolute_error: 1.7643 - val_loss: 4.6094 - val_mean_absolute_error: 1.7662\n",
      "Epoch 68/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.5898 - mean_absolute_error: 1.7638 - val_loss: 4.6493 - val_mean_absolute_error: 1.7816\n",
      "Epoch 69/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5886 - mean_absolute_error: 1.7634 - val_loss: 4.6240 - val_mean_absolute_error: 1.7718\n",
      "Epoch 70/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.5855 - mean_absolute_error: 1.7628 - val_loss: 4.6065 - val_mean_absolute_error: 1.7704\n",
      "Epoch 71/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.5838 - mean_absolute_error: 1.7624 - val_loss: 4.6231 - val_mean_absolute_error: 1.7640\n",
      "Epoch 72/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5812 - mean_absolute_error: 1.7619 - val_loss: 4.6241 - val_mean_absolute_error: 1.7633\n",
      "Epoch 73/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.5800 - mean_absolute_error: 1.7615 - val_loss: 4.6071 - val_mean_absolute_error: 1.7681\n",
      "Epoch 74/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5775 - mean_absolute_error: 1.7611 - val_loss: 4.6252 - val_mean_absolute_error: 1.7613\n",
      "Epoch 75/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.5761 - mean_absolute_error: 1.7607 - val_loss: 4.6214 - val_mean_absolute_error: 1.7604\n",
      "Epoch 76/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5735 - mean_absolute_error: 1.7599 - val_loss: 4.6055 - val_mean_absolute_error: 1.7713\n",
      "Epoch 77/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.5706 - mean_absolute_error: 1.7595 - val_loss: 4.6230 - val_mean_absolute_error: 1.7708\n",
      "Epoch 78/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5701 - mean_absolute_error: 1.7592 - val_loss: 4.5976 - val_mean_absolute_error: 1.7651\n",
      "Epoch 79/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5671 - mean_absolute_error: 1.7585 - val_loss: 4.5898 - val_mean_absolute_error: 1.7646\n",
      "Epoch 80/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5647 - mean_absolute_error: 1.7581 - val_loss: 4.6199 - val_mean_absolute_error: 1.7706\n",
      "Epoch 81/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5633 - mean_absolute_error: 1.7575 - val_loss: 4.6120 - val_mean_absolute_error: 1.7658\n",
      "Epoch 82/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5620 - mean_absolute_error: 1.7572 - val_loss: 4.6026 - val_mean_absolute_error: 1.7717\n",
      "Epoch 83/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5602 - mean_absolute_error: 1.7570 - val_loss: 4.5919 - val_mean_absolute_error: 1.7638\n",
      "Epoch 84/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.5578 - mean_absolute_error: 1.7563 - val_loss: 4.6028 - val_mean_absolute_error: 1.7617\n",
      "Epoch 85/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5557 - mean_absolute_error: 1.7559 - val_loss: 4.5992 - val_mean_absolute_error: 1.7683\n",
      "Epoch 86/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.5534 - mean_absolute_error: 1.7553 - val_loss: 4.6022 - val_mean_absolute_error: 1.7713\n",
      "Epoch 87/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5520 - mean_absolute_error: 1.7550 - val_loss: 4.6042 - val_mean_absolute_error: 1.7636\n",
      "Epoch 88/100\n",
      "44033/44033 [==============================] - 168s 4ms/step - loss: 4.5511 - mean_absolute_error: 1.7547 - val_loss: 4.5831 - val_mean_absolute_error: 1.7586\n",
      "Epoch 89/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5482 - mean_absolute_error: 1.7540 - val_loss: 4.5862 - val_mean_absolute_error: 1.7677\n",
      "Epoch 90/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5470 - mean_absolute_error: 1.7539 - val_loss: 4.5914 - val_mean_absolute_error: 1.7647\n",
      "Epoch 91/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5449 - mean_absolute_error: 1.7532 - val_loss: 4.5862 - val_mean_absolute_error: 1.7576\n",
      "Epoch 92/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5435 - mean_absolute_error: 1.7528 - val_loss: 4.5932 - val_mean_absolute_error: 1.7624\n",
      "Epoch 93/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5420 - mean_absolute_error: 1.7526 - val_loss: 4.5792 - val_mean_absolute_error: 1.7569\n",
      "Epoch 94/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5396 - mean_absolute_error: 1.7522 - val_loss: 4.6431 - val_mean_absolute_error: 1.7572\n",
      "Epoch 95/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5375 - mean_absolute_error: 1.7515 - val_loss: 4.5892 - val_mean_absolute_error: 1.7647\n",
      "Epoch 96/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5359 - mean_absolute_error: 1.7513 - val_loss: 4.6124 - val_mean_absolute_error: 1.7677\n",
      "Epoch 97/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5337 - mean_absolute_error: 1.7507 - val_loss: 4.5856 - val_mean_absolute_error: 1.7625\n",
      "Epoch 98/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5324 - mean_absolute_error: 1.7504 - val_loss: 4.6129 - val_mean_absolute_error: 1.7556\n",
      "Epoch 99/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5305 - mean_absolute_error: 1.7499 - val_loss: 4.5799 - val_mean_absolute_error: 1.7598\n",
      "Epoch 100/100\n",
      "44033/44033 [==============================] - 167s 4ms/step - loss: 4.5292 - mean_absolute_error: 1.7497 - val_loss: 4.6000 - val_mean_absolute_error: 1.7612\n",
      "11009/11009 [==============================] - 23s 2ms/step - loss: 4.6000 - mean_absolute_error: 1.7612\n",
      "Optimizer: SGD\n",
      "Mean Squared Error on Test Data: 4.60\n",
      "Mean Absolute Error on Test Data: 1.76\n",
      "Epoch 1/100\n",
      "44033/44033 [==============================] - 179s 4ms/step - loss: 5.1758 - mean_absolute_error: 1.8796 - val_loss: 4.9740 - val_mean_absolute_error: 1.8497\n",
      "Epoch 2/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.9587 - mean_absolute_error: 1.8480 - val_loss: 4.9535 - val_mean_absolute_error: 1.8261\n",
      "Epoch 3/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.9322 - mean_absolute_error: 1.8427 - val_loss: 4.9058 - val_mean_absolute_error: 1.8428\n",
      "Epoch 4/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.9177 - mean_absolute_error: 1.8395 - val_loss: 4.9093 - val_mean_absolute_error: 1.8462\n",
      "Epoch 5/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8991 - mean_absolute_error: 1.8356 - val_loss: 4.8732 - val_mean_absolute_error: 1.8208\n",
      "Epoch 6/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.8854 - mean_absolute_error: 1.8327 - val_loss: 4.8777 - val_mean_absolute_error: 1.8236\n",
      "Epoch 7/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8740 - mean_absolute_error: 1.8301 - val_loss: 4.8491 - val_mean_absolute_error: 1.8210\n",
      "Epoch 8/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.8670 - mean_absolute_error: 1.8285 - val_loss: 4.8857 - val_mean_absolute_error: 1.8162\n",
      "Epoch 9/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8589 - mean_absolute_error: 1.8266 - val_loss: 4.8343 - val_mean_absolute_error: 1.8163\n",
      "Epoch 10/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.8517 - mean_absolute_error: 1.8251 - val_loss: 4.8508 - val_mean_absolute_error: 1.8197\n",
      "Epoch 11/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8444 - mean_absolute_error: 1.8233 - val_loss: 4.8298 - val_mean_absolute_error: 1.8105\n",
      "Epoch 12/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8369 - mean_absolute_error: 1.8215 - val_loss: 4.8603 - val_mean_absolute_error: 1.8143\n",
      "Epoch 13/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8310 - mean_absolute_error: 1.8201 - val_loss: 4.8117 - val_mean_absolute_error: 1.8163\n",
      "Epoch 14/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8248 - mean_absolute_error: 1.8189 - val_loss: 4.8092 - val_mean_absolute_error: 1.8087\n",
      "Epoch 15/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8211 - mean_absolute_error: 1.8180 - val_loss: 4.8041 - val_mean_absolute_error: 1.8133\n",
      "Epoch 16/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8163 - mean_absolute_error: 1.8169 - val_loss: 4.7988 - val_mean_absolute_error: 1.8107\n",
      "Epoch 17/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8131 - mean_absolute_error: 1.8162 - val_loss: 4.7969 - val_mean_absolute_error: 1.8122\n",
      "Epoch 18/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8088 - mean_absolute_error: 1.8153 - val_loss: 4.7976 - val_mean_absolute_error: 1.8110\n",
      "Epoch 19/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.8059 - mean_absolute_error: 1.8143 - val_loss: 4.7965 - val_mean_absolute_error: 1.8050\n",
      "Epoch 20/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.8021 - mean_absolute_error: 1.8135 - val_loss: 4.8007 - val_mean_absolute_error: 1.8192\n",
      "Epoch 21/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7990 - mean_absolute_error: 1.8129 - val_loss: 4.7902 - val_mean_absolute_error: 1.8027\n",
      "Epoch 22/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7944 - mean_absolute_error: 1.8118 - val_loss: 4.7837 - val_mean_absolute_error: 1.8112\n",
      "Epoch 23/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7910 - mean_absolute_error: 1.8109 - val_loss: 4.7945 - val_mean_absolute_error: 1.8057\n",
      "Epoch 24/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7864 - mean_absolute_error: 1.8100 - val_loss: 4.7783 - val_mean_absolute_error: 1.8084\n",
      "Epoch 25/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7836 - mean_absolute_error: 1.8094 - val_loss: 4.7691 - val_mean_absolute_error: 1.8051\n",
      "Epoch 26/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7796 - mean_absolute_error: 1.8085 - val_loss: 4.7875 - val_mean_absolute_error: 1.8092\n",
      "Epoch 27/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7764 - mean_absolute_error: 1.8076 - val_loss: 4.7659 - val_mean_absolute_error: 1.8025\n",
      "Epoch 28/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7738 - mean_absolute_error: 1.8069 - val_loss: 4.7792 - val_mean_absolute_error: 1.8135\n",
      "Epoch 29/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7718 - mean_absolute_error: 1.8065 - val_loss: 4.7528 - val_mean_absolute_error: 1.7993\n",
      "Epoch 30/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7696 - mean_absolute_error: 1.8058 - val_loss: 4.7757 - val_mean_absolute_error: 1.8045\n",
      "Epoch 31/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7672 - mean_absolute_error: 1.8054 - val_loss: 4.7566 - val_mean_absolute_error: 1.8150\n",
      "Epoch 32/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7652 - mean_absolute_error: 1.8048 - val_loss: 4.7945 - val_mean_absolute_error: 1.7988\n",
      "Epoch 33/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7646 - mean_absolute_error: 1.8050 - val_loss: 4.7487 - val_mean_absolute_error: 1.8057\n",
      "Epoch 34/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7611 - mean_absolute_error: 1.8041 - val_loss: 4.7487 - val_mean_absolute_error: 1.8059\n",
      "Epoch 35/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7584 - mean_absolute_error: 1.8035 - val_loss: 4.7647 - val_mean_absolute_error: 1.7958\n",
      "Epoch 36/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7574 - mean_absolute_error: 1.8031 - val_loss: 4.7572 - val_mean_absolute_error: 1.8066\n",
      "Epoch 37/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7559 - mean_absolute_error: 1.8029 - val_loss: 4.7465 - val_mean_absolute_error: 1.8088\n",
      "Epoch 38/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7550 - mean_absolute_error: 1.8025 - val_loss: 4.7691 - val_mean_absolute_error: 1.7986\n",
      "Epoch 39/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7531 - mean_absolute_error: 1.8021 - val_loss: 4.7543 - val_mean_absolute_error: 1.8132\n",
      "Epoch 40/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.7511 - mean_absolute_error: 1.8015 - val_loss: 4.7701 - val_mean_absolute_error: 1.7942\n",
      "Epoch 41/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7490 - mean_absolute_error: 1.8011 - val_loss: 4.7547 - val_mean_absolute_error: 1.8026\n",
      "Epoch 42/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7496 - mean_absolute_error: 1.8014 - val_loss: 4.7488 - val_mean_absolute_error: 1.7884\n",
      "Epoch 43/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7477 - mean_absolute_error: 1.8008 - val_loss: 4.7519 - val_mean_absolute_error: 1.7985\n",
      "Epoch 44/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7477 - mean_absolute_error: 1.8010 - val_loss: 4.7418 - val_mean_absolute_error: 1.7933\n",
      "Epoch 45/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7448 - mean_absolute_error: 1.8001 - val_loss: 4.7491 - val_mean_absolute_error: 1.7996\n",
      "Epoch 46/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7440 - mean_absolute_error: 1.8000 - val_loss: 4.7425 - val_mean_absolute_error: 1.7928\n",
      "Epoch 47/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7430 - mean_absolute_error: 1.7998 - val_loss: 4.7679 - val_mean_absolute_error: 1.7961\n",
      "Epoch 48/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7410 - mean_absolute_error: 1.7993 - val_loss: 4.7580 - val_mean_absolute_error: 1.8132\n",
      "Epoch 49/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7407 - mean_absolute_error: 1.7994 - val_loss: 4.7439 - val_mean_absolute_error: 1.8041\n",
      "Epoch 50/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7391 - mean_absolute_error: 1.7989 - val_loss: 4.7276 - val_mean_absolute_error: 1.7932\n",
      "Epoch 51/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7386 - mean_absolute_error: 1.7989 - val_loss: 4.7354 - val_mean_absolute_error: 1.7987\n",
      "Epoch 52/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7375 - mean_absolute_error: 1.7986 - val_loss: 4.7358 - val_mean_absolute_error: 1.8069\n",
      "Epoch 53/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7365 - mean_absolute_error: 1.7983 - val_loss: 4.7350 - val_mean_absolute_error: 1.7949\n",
      "Epoch 54/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7359 - mean_absolute_error: 1.7980 - val_loss: 4.7692 - val_mean_absolute_error: 1.8011\n",
      "Epoch 55/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7332 - mean_absolute_error: 1.7975 - val_loss: 4.7315 - val_mean_absolute_error: 1.7890\n",
      "Epoch 56/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7324 - mean_absolute_error: 1.7974 - val_loss: 4.7333 - val_mean_absolute_error: 1.7935\n",
      "Epoch 57/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7325 - mean_absolute_error: 1.7975 - val_loss: 4.7415 - val_mean_absolute_error: 1.7855\n",
      "Epoch 58/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7302 - mean_absolute_error: 1.7969 - val_loss: 4.7311 - val_mean_absolute_error: 1.8003\n",
      "Epoch 59/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7294 - mean_absolute_error: 1.7967 - val_loss: 4.7373 - val_mean_absolute_error: 1.8027\n",
      "Epoch 60/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7277 - mean_absolute_error: 1.7964 - val_loss: 4.7229 - val_mean_absolute_error: 1.7976\n",
      "Epoch 61/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.7270 - mean_absolute_error: 1.7962 - val_loss: 4.7284 - val_mean_absolute_error: 1.7914\n",
      "Epoch 62/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7267 - mean_absolute_error: 1.7960 - val_loss: 4.7367 - val_mean_absolute_error: 1.7968\n",
      "Epoch 63/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.7249 - mean_absolute_error: 1.7956 - val_loss: 4.7238 - val_mean_absolute_error: 1.7878\n",
      "Epoch 64/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7240 - mean_absolute_error: 1.7956 - val_loss: 4.7190 - val_mean_absolute_error: 1.7947\n",
      "Epoch 65/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7241 - mean_absolute_error: 1.7953 - val_loss: 4.7153 - val_mean_absolute_error: 1.7953\n",
      "Epoch 66/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7217 - mean_absolute_error: 1.7949 - val_loss: 4.7234 - val_mean_absolute_error: 1.8001\n",
      "Epoch 67/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7212 - mean_absolute_error: 1.7947 - val_loss: 4.7127 - val_mean_absolute_error: 1.7882\n",
      "Epoch 68/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7194 - mean_absolute_error: 1.7943 - val_loss: 4.7232 - val_mean_absolute_error: 1.7917\n",
      "Epoch 69/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7200 - mean_absolute_error: 1.7943 - val_loss: 4.7187 - val_mean_absolute_error: 1.7964\n",
      "Epoch 70/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7202 - mean_absolute_error: 1.7944 - val_loss: 4.7284 - val_mean_absolute_error: 1.7882\n",
      "Epoch 71/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7194 - mean_absolute_error: 1.7942 - val_loss: 4.7190 - val_mean_absolute_error: 1.8039\n",
      "Epoch 72/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7161 - mean_absolute_error: 1.7936 - val_loss: 4.7102 - val_mean_absolute_error: 1.7964\n",
      "Epoch 73/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7170 - mean_absolute_error: 1.7938 - val_loss: 4.7338 - val_mean_absolute_error: 1.7901\n",
      "Epoch 74/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7154 - mean_absolute_error: 1.7933 - val_loss: 4.7840 - val_mean_absolute_error: 1.7837\n",
      "Epoch 75/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7147 - mean_absolute_error: 1.7933 - val_loss: 4.7092 - val_mean_absolute_error: 1.7930\n",
      "Epoch 76/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7138 - mean_absolute_error: 1.7929 - val_loss: 4.7120 - val_mean_absolute_error: 1.7830\n",
      "Epoch 77/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7125 - mean_absolute_error: 1.7928 - val_loss: 4.7260 - val_mean_absolute_error: 1.8044\n",
      "Epoch 78/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7129 - mean_absolute_error: 1.7927 - val_loss: 4.7545 - val_mean_absolute_error: 1.7917\n",
      "Epoch 79/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7094 - mean_absolute_error: 1.7922 - val_loss: 4.7150 - val_mean_absolute_error: 1.7971\n",
      "Epoch 80/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7091 - mean_absolute_error: 1.7919 - val_loss: 4.7158 - val_mean_absolute_error: 1.7863\n",
      "Epoch 81/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7068 - mean_absolute_error: 1.7913 - val_loss: 4.6991 - val_mean_absolute_error: 1.7895\n",
      "Epoch 82/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7069 - mean_absolute_error: 1.7914 - val_loss: 4.7041 - val_mean_absolute_error: 1.7897\n",
      "Epoch 83/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7063 - mean_absolute_error: 1.7913 - val_loss: 4.7198 - val_mean_absolute_error: 1.7954\n",
      "Epoch 84/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.7049 - mean_absolute_error: 1.7908 - val_loss: 4.7166 - val_mean_absolute_error: 1.7951\n",
      "Epoch 85/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7040 - mean_absolute_error: 1.7908 - val_loss: 4.6977 - val_mean_absolute_error: 1.7909\n",
      "Epoch 86/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7029 - mean_absolute_error: 1.7907 - val_loss: 4.7103 - val_mean_absolute_error: 1.7961\n",
      "Epoch 87/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7035 - mean_absolute_error: 1.7907 - val_loss: 4.7079 - val_mean_absolute_error: 1.7928\n",
      "Epoch 88/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7020 - mean_absolute_error: 1.7903 - val_loss: 4.7064 - val_mean_absolute_error: 1.7778\n",
      "Epoch 89/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7026 - mean_absolute_error: 1.7904 - val_loss: 4.7174 - val_mean_absolute_error: 1.7875\n",
      "Epoch 90/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.7002 - mean_absolute_error: 1.7898 - val_loss: 4.7015 - val_mean_absolute_error: 1.7908\n",
      "Epoch 91/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.6987 - mean_absolute_error: 1.7895 - val_loss: 4.7125 - val_mean_absolute_error: 1.7876\n",
      "Epoch 92/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.6982 - mean_absolute_error: 1.7895 - val_loss: 4.7065 - val_mean_absolute_error: 1.7851\n",
      "Epoch 93/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.6977 - mean_absolute_error: 1.7893 - val_loss: 4.7091 - val_mean_absolute_error: 1.7893\n",
      "Epoch 94/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.6959 - mean_absolute_error: 1.7890 - val_loss: 4.6898 - val_mean_absolute_error: 1.7952\n",
      "Epoch 95/100\n",
      "44033/44033 [==============================] - 176s 4ms/step - loss: 4.6953 - mean_absolute_error: 1.7888 - val_loss: 4.6919 - val_mean_absolute_error: 1.7865\n",
      "Epoch 96/100\n",
      "44033/44033 [==============================] - 174s 4ms/step - loss: 4.6946 - mean_absolute_error: 1.7886 - val_loss: 4.6961 - val_mean_absolute_error: 1.7833\n",
      "Epoch 97/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.6950 - mean_absolute_error: 1.7888 - val_loss: 4.6874 - val_mean_absolute_error: 1.7794\n",
      "Epoch 98/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.6942 - mean_absolute_error: 1.7883 - val_loss: 4.6963 - val_mean_absolute_error: 1.7917\n",
      "Epoch 99/100\n",
      "44033/44033 [==============================] - 174s 4ms/step - loss: 4.6933 - mean_absolute_error: 1.7883 - val_loss: 4.7041 - val_mean_absolute_error: 1.7820\n",
      "Epoch 100/100\n",
      "44033/44033 [==============================] - 175s 4ms/step - loss: 4.6930 - mean_absolute_error: 1.7883 - val_loss: 4.7046 - val_mean_absolute_error: 1.7912\n",
      "11009/11009 [==============================] - 23s 2ms/step - loss: 4.7046 - mean_absolute_error: 1.7912\n",
      "Optimizer: Adam\n",
      "Mean Squared Error on Test Data: 4.70\n",
      "Mean Absolute Error on Test Data: 1.79\n",
      "Epoch 1/100\n",
      "44033/44033 [==============================] - 174s 4ms/step - loss: 5.2506 - mean_absolute_error: 1.8913 - val_loss: 4.9396 - val_mean_absolute_error: 1.8491\n",
      "Epoch 2/100\n",
      "44033/44033 [==============================] - 171s 4ms/step - loss: 4.9707 - mean_absolute_error: 1.8494 - val_loss: 4.9381 - val_mean_absolute_error: 1.8520\n",
      "Epoch 3/100\n",
      "44033/44033 [==============================] - 170s 4ms/step - loss: 4.9471 - mean_absolute_error: 1.8447 - val_loss: 4.9378 - val_mean_absolute_error: 1.8340\n",
      "Epoch 4/100\n",
      "44033/44033 [==============================] - 170s 4ms/step - loss: 4.9284 - mean_absolute_error: 1.8409 - val_loss: 4.9295 - val_mean_absolute_error: 1.8305\n",
      "Epoch 5/100\n",
      "44033/44033 [==============================] - 171s 4ms/step - loss: 4.9200 - mean_absolute_error: 1.8391 - val_loss: 4.9013 - val_mean_absolute_error: 1.8198\n",
      "Epoch 6/100\n",
      "44033/44033 [==============================] - 172s 4ms/step - loss: 4.9140 - mean_absolute_error: 1.8382 - val_loss: 4.9017 - val_mean_absolute_error: 1.8389\n",
      "Epoch 7/100\n",
      "44033/44033 [==============================] - 173s 4ms/step - loss: 4.9090 - mean_absolute_error: 1.8373 - val_loss: 4.8827 - val_mean_absolute_error: 1.8373\n",
      "Epoch 8/100\n",
      "44033/44033 [==============================] - 172s 4ms/step - loss: 4.9067 - mean_absolute_error: 1.8369 - val_loss: 4.8851 - val_mean_absolute_error: 1.8417\n",
      "Epoch 9/100\n",
      "44033/44033 [==============================] - 173s 4ms/step - loss: 4.9091 - mean_absolute_error: 1.8372 - val_loss: 4.9006 - val_mean_absolute_error: 1.8309\n",
      "Epoch 10/100\n",
      "44033/44033 [==============================] - 172s 4ms/step - loss: 4.9061 - mean_absolute_error: 1.8367 - val_loss: 4.8907 - val_mean_absolute_error: 1.8414\n",
      "Epoch 11/100\n",
      "44033/44033 [==============================] - 173s 4ms/step - loss: 4.9053 - mean_absolute_error: 1.8364 - val_loss: 4.8961 - val_mean_absolute_error: 1.8514\n",
      "Epoch 12/100\n",
      "44033/44033 [==============================] - 173s 4ms/step - loss: 4.9010 - mean_absolute_error: 1.8355 - val_loss: 4.8928 - val_mean_absolute_error: 1.8364\n",
      "Epoch 13/100\n",
      "44033/44033 [==============================] - 173s 4ms/step - loss: 4.9023 - mean_absolute_error: 1.8355 - val_loss: 4.8860 - val_mean_absolute_error: 1.8447\n",
      "Epoch 14/100\n",
      "44033/44033 [==============================] - 177s 4ms/step - loss: 4.9064 - mean_absolute_error: 1.8363 - val_loss: 4.9186 - val_mean_absolute_error: 1.8358\n",
      "Epoch 15/100\n",
      "44033/44033 [==============================] - 179s 4ms/step - loss: 4.9043 - mean_absolute_error: 1.8358 - val_loss: 4.8922 - val_mean_absolute_error: 1.8272\n",
      "Epoch 16/100\n",
      "44033/44033 [==============================] - 184s 4ms/step - loss: 4.9035 - mean_absolute_error: 1.8358 - val_loss: 4.9325 - val_mean_absolute_error: 1.8301\n",
      "Epoch 17/100\n",
      "44033/44033 [==============================] - 183s 4ms/step - loss: 4.9081 - mean_absolute_error: 1.8367 - val_loss: 4.8869 - val_mean_absolute_error: 1.8182\n",
      "Epoch 17: early stopping\n",
      "11009/11009 [==============================] - 25s 2ms/step - loss: 4.8869 - mean_absolute_error: 1.8182\n",
      "Optimizer: RMSprop\n",
      "Mean Squared Error on Test Data: 4.89\n",
      "Mean Absolute Error on Test Data: 1.82\n",
      "Training stopped early at epoch 16 to prevent overfitting.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Standardize your input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "optimizers = [SGD(learning_rate=0.001), Adam(learning_rate=0.001), RMSprop(learning_rate=0.001)]\n",
    "\n",
    "# Define the early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    # Create a Sequential model\n",
    "    model = Sequential([\n",
    "                Dense(1024, activation='relu', input_dim=X_train.shape[1]),\n",
    "                Dense(512, activation='relu'),\n",
    "                Dense(256, activation='relu'),\n",
    "                Dense(128, activation='relu'),\n",
    "                Dense(64, activation='relu'),\n",
    "                Dense(1)\n",
    "            ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "    # Train the model with early stopping callback\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(f\"Optimizer: {optimizer.get_config()['name']}\")\n",
    "    print(f\"Mean Squared Error on Test Data: {loss:.2f}\")\n",
    "    print(f\"Mean Absolute Error on Test Data: {mae:.2f}\")\n",
    "\n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        print(f\"Training stopped early at epoch {early_stopping.stopped_epoch} to prevent overfitting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeeb2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
